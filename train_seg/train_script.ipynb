{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cb64ff4-5620-43c0-9d0d-04d97f7bd273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fbd62a-1f38-4895-b4cc-dbf89a013af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "import torch\n",
    "from mmcv.runner import init_dist\n",
    "from mmcv.utils import Config, DictAction, get_git_hash\n",
    "\n",
    "from mmseg import __version__\n",
    "from mmseg.apis import set_random_seed, train_segmentor\n",
    "from mmseg.datasets import build_dataset\n",
    "from mmseg.models import build_segmentor\n",
    "from mmseg.utils import collect_env, get_root_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "547d715f-b70c-40c5-beb3-6f82b7eae944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROOT_DIR = os.path.dirname(globals()['_dh'][0])\n",
    "#sys.path.append(ROOT_DIR)\n",
    "#print(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd1cbaba-6d44-4444-aeb3-ffa010ca008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = \"configs/foodnet/SETR_Naive_512x512_80k_base.py\"\n",
    "WORK_DIR = \"checkpoints/SETR_Naive_ReLeM\"\n",
    "DETERMINISTIC = False\n",
    "SEED = 42\n",
    "DATA_ROOT = \"D:/_RAW_DATASET/FoodSeg103/FoodSeg103/Images\"\n",
    "SPLITS = \"D:/_RAW_DATASET/FoodSeg103/FoodSeg103/ImageSets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93ce34e4-fd60-4ed1-b433-0d76949f635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3982e7e2-f96f-45f1-b4d9-b14f56acd7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.datasets.builder import DATASETS\n",
    "from mmseg.datasets.custom import CustomDataset\n",
    "\n",
    "food_classes = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,\n",
    "               21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n",
    "               41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n",
    "               61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n",
    "               81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100,\n",
    "               101, 102, 103)\n",
    "\n",
    "food_palette =[[0, 0, 0], [40, 100, 150], [80, 150, 200], [120, 200, 10], [160, 10, 60],\n",
    "               [200, 60, 110], [0, 110, 160], [40, 160, 210], [80, 210, 20], [120, 20, 70],\n",
    "               [160, 70, 120], [200, 120, 170], [0, 170, 220], [40, 220, 30], [80, 30, 80],\n",
    "               [120, 80, 130], [160, 130, 180], [200, 180, 230], [0, 230, 40], [40, 40, 90],\n",
    "               [80, 90, 140], [120, 140, 190], [160, 190, 0], [200, 0, 50], [0, 50, 100],\n",
    "               [40, 100, 150], [80, 150, 200], [120, 200, 10], [160, 10, 60], [200, 60, 110],\n",
    "               [0, 110, 160], [40, 160, 210], [80, 210, 20], [120, 20, 70], [160, 70, 120],\n",
    "               [200, 120, 170], [0, 170, 220], [40, 220, 30], [80, 30, 80], [120, 80, 130],\n",
    "               [160, 130, 180], [200, 180, 230], [0, 230, 40], [40, 40, 90], [80, 90, 140],\n",
    "               [120, 140, 190], [160, 190, 0], [200, 0, 50], [0, 50, 100], [40, 100, 150],\n",
    "               [80, 150, 200], [120, 200, 10], [160, 10, 60], [200, 60, 110], [0, 110, 160],\n",
    "               [40, 160, 210], [80, 210, 20], [120, 20, 70], [160, 70, 120], [200, 120, 170],\n",
    "               [0, 170, 220], [40, 220, 30], [80, 30, 80], [120, 80, 130], [160, 130, 180],\n",
    "               [200, 180, 230], [0, 230, 40], [40, 40, 90], [80, 90, 140], [120, 140, 190],\n",
    "               [160, 190, 0], [200, 0, 50], [0, 50, 100], [40, 100, 150], [80, 150, 200],\n",
    "               [120, 200, 10], [160, 10, 60], [200, 60, 110], [0, 110, 160], [40, 160, 210],\n",
    "               [80, 210, 20], [120, 20, 70], [160, 70, 120], [200, 120, 170], [0, 170, 220],\n",
    "               [40, 220, 30], [80, 30, 80], [120, 80, 130], [160, 130, 180], [200, 180, 230],\n",
    "               [0, 230, 40], [40, 40, 90], [80, 90, 140], [120, 140, 190], [160, 190, 0],\n",
    "               [200, 0, 50], [0, 50, 100], [40, 100, 150], [80, 150, 200], [120, 200, 10],\n",
    "               [160, 10, 60], [200, 60, 110], [0, 110, 160], [40, 160, 210]]\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class FoodBackgroundDataset(CustomDataset):\n",
    "    CLASSES = food_classes\n",
    "    PALETTE = food_palette\n",
    "    def __init__(self, split, **kwargs):\n",
    "        super().__init__(img_suffix='.jpg', seg_map_suffix='.png', **kwargs)\n",
    "        assert os.path.exists(self.img_dir) and os.path.exists(self.ann_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43bad928-0b11-4ff0-9b79-b894d0caf723",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.dataset_type = 'FoodBackgroundDataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "814a7827-5d87-40f4-8793-883416290324",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data = dict(\n",
    "    samples_per_gpu=1,\n",
    "    workers_per_gpu=1,\n",
    "    train=dict(\n",
    "        type=cfg.dataset_type,\n",
    "        data_root=DATA_ROOT,\n",
    "        img_dir='img_dir/train',\n",
    "        ann_dir='ann_dir/train',\n",
    "        pipeline=cfg.train_pipeline,\n",
    "        split=SPLITS + \"/train.txt\"\n",
    "    ),\n",
    "    val=dict(\n",
    "        type=cfg.dataset_type,\n",
    "        data_root=DATA_ROOT,\n",
    "        img_dir='img_dir/test',\n",
    "        ann_dir='ann_dir/test',\n",
    "        pipeline=cfg.test_pipeline,\n",
    "        split=SPLITS + \"/test.txt\"\n",
    "    ),\n",
    "    test=dict(\n",
    "        type=cfg.dataset_type,\n",
    "        data_root=DATA_ROOT,\n",
    "        img_dir='img_dir/test',\n",
    "        ann_dir='ann_dir/test',\n",
    "        pipeline=cfg.test_pipeline,\n",
    "        split=SPLITS + \"/test.txt\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a403595-a4a5-4822-8f23-25ae67f681f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.get('cudnn_benchmark', False):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "cfg.work_dir = WORK_DIR\n",
    "cfg.gpu_ids = range(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3705244-91d3-4815-b2c7-a4136ced1968",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8509738f-1291-47de-9df0-203458af428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcv.mkdir_or_exist(os.path.abspath(cfg.work_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f2ce1ea-f9b8-49fb-9c7d-0497c2c9f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.dump(os.path.join(cfg.work_dir, os.path.basename(CONFIG)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "952df344-cb6d-459a-8a5d-055bb329e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
    "log_file = os.path.join(cfg.work_dir, f'{timestamp}.log')\n",
    "logger = get_root_logger(log_file=log_file, log_level=cfg.log_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "076ddb26-6c3c-4cdc-a01a-b0abc76f79c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 17:29:26,506 - mmseg - INFO - Environment info:\n",
      "------------------------------------------------------------\n",
      "sys.platform: win32\n",
      "Python: 3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]\n",
      "CUDA available: True\n",
      "GPU 0: NVIDIA GeForce RTX 2070 SUPER\n",
      "CUDA_HOME: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.3\n",
      "NVCC: Not Available\n",
      "GCC: n/a\n",
      "PyTorch: 1.10.0\n",
      "PyTorch compiling details: PyTorch built with:\n",
      "  - C++ Version: 199711\n",
      "  - MSVC 192829337\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n",
      "  - OpenMP 2019\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.3\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.2\n",
      "  - Magma 2.5.4\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=C:/cb/pytorch_1000000000000/work/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj -DUSE_PTHREADPOOL -openmp:experimental -IC:/cb/pytorch_1000000000000/work/mkl/include -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, \n",
      "\n",
      "TorchVision: 0.11.1\n",
      "OpenCV: 4.0.1\n",
      "MMCV: 1.4.4\n",
      "MMCV Compiler: MSVC 192829334\n",
      "MMCV CUDA Compiler: 11.3\n",
      "MMSegmentation: 0.21.1+6bc82ec\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# init the meta dict to record some important information such as\n",
    "    # environment info and seed, which will be logged\n",
    "meta = dict()\n",
    "# log env info\n",
    "env_info_dict = collect_env()\n",
    "env_info = '\\n'.join([f'{k}: {v}' for k, v in env_info_dict.items()])\n",
    "dash_line = '-' * 60 + '\\n'\n",
    "logger.info('Environment info:\\n' + dash_line + env_info + '\\n' + dash_line)\n",
    "meta['env_info'] = env_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b450cb1-6f36-4a4b-9a8c-d1c996a59a49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 17:29:26,524 - mmseg - INFO - Distributed training: False\n",
      "2022-02-11 17:29:26,816 - mmseg - INFO - Config:\n",
      "backbone_norm_cfg = dict(type='LN', eps=1e-06, requires_grad=True)\n",
      "norm_cfg = dict(type='SyncBN', requires_grad=True)\n",
      "model = dict(\n",
      "    type='EncoderDecoder',\n",
      "    pretrained=None,\n",
      "    backbone=dict(\n",
      "        type='VisionTransformer',\n",
      "        img_size=(512, 512),\n",
      "        patch_size=16,\n",
      "        in_channels=3,\n",
      "        embed_dims=1024,\n",
      "        num_layers=24,\n",
      "        num_heads=16,\n",
      "        out_indices=(9, 14, 19, 23),\n",
      "        drop_rate=0.0,\n",
      "        norm_cfg=dict(type='LN', eps=1e-06, requires_grad=True),\n",
      "        with_cls_token=True,\n",
      "        interpolate_mode='bilinear',\n",
      "        init_cfg=dict(type='Pretrained', checkpoint='mmcls://vit_large_p16')),\n",
      "    decode_head=dict(\n",
      "        type='SETRUPHead',\n",
      "        in_channels=1024,\n",
      "        channels=256,\n",
      "        in_index=3,\n",
      "        num_classes=104,\n",
      "        dropout_ratio=0,\n",
      "        norm_cfg=dict(type='SyncBN', requires_grad=True),\n",
      "        num_convs=1,\n",
      "        up_scale=4,\n",
      "        kernel_size=1,\n",
      "        align_corners=False,\n",
      "        loss_decode=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),\n",
      "    auxiliary_head=[\n",
      "        dict(\n",
      "            type='SETRUPHead',\n",
      "            in_channels=1024,\n",
      "            channels=256,\n",
      "            in_index=0,\n",
      "            num_classes=104,\n",
      "            dropout_ratio=0,\n",
      "            norm_cfg=dict(type='SyncBN', requires_grad=True),\n",
      "            act_cfg=dict(type='ReLU'),\n",
      "            num_convs=2,\n",
      "            kernel_size=1,\n",
      "            align_corners=False,\n",
      "            loss_decode=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.4)),\n",
      "        dict(\n",
      "            type='SETRUPHead',\n",
      "            in_channels=1024,\n",
      "            channels=256,\n",
      "            in_index=1,\n",
      "            num_classes=104,\n",
      "            dropout_ratio=0,\n",
      "            norm_cfg=dict(type='SyncBN', requires_grad=True),\n",
      "            act_cfg=dict(type='ReLU'),\n",
      "            num_convs=2,\n",
      "            kernel_size=1,\n",
      "            align_corners=False,\n",
      "            loss_decode=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.4)),\n",
      "        dict(\n",
      "            type='SETRUPHead',\n",
      "            in_channels=1024,\n",
      "            channels=256,\n",
      "            in_index=2,\n",
      "            num_classes=104,\n",
      "            dropout_ratio=0,\n",
      "            norm_cfg=dict(type='SyncBN', requires_grad=True),\n",
      "            act_cfg=dict(type='ReLU'),\n",
      "            num_convs=2,\n",
      "            kernel_size=1,\n",
      "            align_corners=False,\n",
      "            loss_decode=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=0.4))\n",
      "    ],\n",
      "    train_cfg=dict(),\n",
      "    test_cfg=dict(mode='slide', crop_size=(512, 512), stride=(341, 341)))\n",
      "dataset_type = 'FoodBackgroundDataset'\n",
      "data_root = './data/FoodSeg103/Images'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "crop_size = (512, 512)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations'),\n",
      "    dict(type='Resize', img_scale=(2049, 1025), ratio_range=(0.5, 2.0)),\n",
      "    dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),\n",
      "    dict(type='RandomFlip', prob=0.5),\n",
      "    dict(type='PhotoMetricDistortion'),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(2049, 1025),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=1,\n",
      "    workers_per_gpu=1,\n",
      "    train=dict(\n",
      "        type='FoodBackgroundDataset',\n",
      "        data_root='D:/_RAW_DATASET/FoodSeg103/FoodSeg103/Images',\n",
      "        img_dir='img_dir/train',\n",
      "        ann_dir='ann_dir/train',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(\n",
      "                type='Resize', img_scale=(2049, 1025), ratio_range=(0.5, 2.0)),\n",
      "            dict(type='RandomCrop', crop_size=(512, 512), cat_max_ratio=0.75),\n",
      "            dict(type='RandomFlip', prob=0.5),\n",
      "            dict(type='PhotoMetricDistortion'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size=(512, 512), pad_val=0, seg_pad_val=255),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(type='Collect', keys=['img', 'gt_semantic_seg'])\n",
      "        ],\n",
      "        split='D:/_RAW_DATASET/FoodSeg103/FoodSeg103/ImageSets/train.txt'),\n",
      "    val=dict(\n",
      "        type='FoodBackgroundDataset',\n",
      "        data_root='D:/_RAW_DATASET/FoodSeg103/FoodSeg103/Images',\n",
      "        img_dir='img_dir/test',\n",
      "        ann_dir='ann_dir/test',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(2049, 1025),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        split='D:/_RAW_DATASET/FoodSeg103/FoodSeg103/ImageSets/test.txt'),\n",
      "    test=dict(\n",
      "        type='FoodBackgroundDataset',\n",
      "        data_root='D:/_RAW_DATASET/FoodSeg103/FoodSeg103/Images',\n",
      "        img_dir='img_dir/test',\n",
      "        ann_dir='ann_dir/test',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(2049, 1025),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        split='D:/_RAW_DATASET/FoodSeg103/FoodSeg103/ImageSets/test.txt'))\n",
      "log_config = dict(\n",
      "    interval=50, hooks=[dict(type='TextLoggerHook', by_epoch=False)])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "cudnn_benchmark = True\n",
      "optimizer = dict(\n",
      "    type='SGD',\n",
      "    lr=0.01,\n",
      "    momentum=0.9,\n",
      "    weight_decay=0.0,\n",
      "    paramwise_cfg=dict(custom_keys=dict(head=dict(lr_mult=10.0))))\n",
      "optimizer_config = dict()\n",
      "lr_config = dict(policy='poly', power=0.9, min_lr=0.0001, by_epoch=False)\n",
      "runner = dict(type='IterBasedRunner', max_iters=80000)\n",
      "checkpoint_config = dict(by_epoch=False, interval=8000)\n",
      "evaluation = dict(interval=8000, metric='mIoU', pre_eval=True)\n",
      "work_dir = 'checkpoints/SETR_Naive_ReLeM'\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Distributed training: {distributed}')\n",
    "logger.info(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec0d0454-f98c-4aba-ac04-7019a02b3243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 17:29:26,821 - mmseg - INFO - Set random seed to 42, deterministic: False\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Set random seed to {SEED}, deterministic: 'f'{DETERMINISTIC}')\n",
    "set_random_seed(SEED, deterministic=DETERMINISTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ee14130-16a4-4095-8dee-382290b1d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.seed = SEED\n",
    "meta['seed'] = SEED\n",
    "meta['exp_name'] = os.path.basename(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc5d0152-b33e-4ca3-b003-e5bb7b404d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_segmentor(\n",
    "        cfg.model,\n",
    "        train_cfg=cfg.get('train_cfg'),\n",
    "        test_cfg=cfg.get('test_cfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0934fa9d-4a6c-42c8-a2b5-12334a6f0ce0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 17:29:28,144 - mmseg - INFO - EncoderDecoder(\n",
      "  (backbone): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (adap_padding): AdaptivePadding()\n",
      "      (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
      "    )\n",
      "    (drop_after_pos): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (6): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (7): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (8): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (9): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (10): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (11): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (12): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (13): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (14): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (15): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (16): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (17): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (18): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (19): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (20): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (21): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (22): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "      (23): TransformerEncoderLayer(\n",
      "        (ln1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MultiheadAttention(\n",
      "          (attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "        (ln2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (activate): GELU()\n",
      "          (layers): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (1): GELU()\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (1): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (dropout_layer): DropPath()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  init_cfg={'type': 'Pretrained', 'checkpoint': 'mmcls://vit_large_p16'}\n",
      "  (decode_head): SETRUPHead(\n",
      "    input_transform=None, ignore_index=255, align_corners=False\n",
      "    (loss_decode): CrossEntropyLoss()\n",
      "    (conv_seg): Conv2d(256, 104, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (up_convs): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (0): ConvModule(\n",
      "          (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (activate): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Upsample()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  init_cfg=[{'type': 'Constant', 'val': 1.0, 'bias': 0, 'layer': 'LayerNorm'}, {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}]\n",
      "  (auxiliary_head): ModuleList(\n",
      "    (0): SETRUPHead(\n",
      "      input_transform=None, ignore_index=255, align_corners=False\n",
      "      (loss_decode): CrossEntropyLoss()\n",
      "      (conv_seg): Conv2d(256, 104, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (up_convs): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): ConvModule(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activate): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Upsample()\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): ConvModule(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activate): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Upsample()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    init_cfg=[{'type': 'Constant', 'val': 1.0, 'bias': 0, 'layer': 'LayerNorm'}, {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}]\n",
      "    (1): SETRUPHead(\n",
      "      input_transform=None, ignore_index=255, align_corners=False\n",
      "      (loss_decode): CrossEntropyLoss()\n",
      "      (conv_seg): Conv2d(256, 104, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (up_convs): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): ConvModule(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activate): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Upsample()\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): ConvModule(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activate): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Upsample()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    init_cfg=[{'type': 'Constant', 'val': 1.0, 'bias': 0, 'layer': 'LayerNorm'}, {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}]\n",
      "    (2): SETRUPHead(\n",
      "      input_transform=None, ignore_index=255, align_corners=False\n",
      "      (loss_decode): CrossEntropyLoss()\n",
      "      (conv_seg): Conv2d(256, 104, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "      (up_convs): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): ConvModule(\n",
      "            (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activate): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Upsample()\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): ConvModule(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (activate): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Upsample()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    init_cfg=[{'type': 'Constant', 'val': 1.0, 'bias': 0, 'layer': 'LayerNorm'}, {'type': 'Normal', 'std': 0.01, 'override': {'name': 'conv_seg'}}]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "logger.info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a3ab437-1cb7-4082-a977-d2817cf5d5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-11 17:29:28,347 - mmseg - INFO - Loaded 4983 images\n"
     ]
    }
   ],
   "source": [
    "datasets = [build_dataset(cfg.data.train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "520a7bb4-e7b8-416c-aadf-cda980ebb7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.FoodBackgroundDataset object at 0x0000020AF7B91460>\n"
     ]
    }
   ],
   "source": [
    "print(datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "520e79d8-6d25-4395-811b-cc37b9af9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(cfg.workflow) == 2:\n",
    "    val_dataset = copy.deepcopy(cfg.data.val)\n",
    "    val_dataset.pipeline = cfg.data.train.pipeline\n",
    "    datasets.append(build_dataset(val_dataset))\n",
    "    if cfg.checkpoint_config is not None:\n",
    "        # save mmseg version, config file content and class names in\n",
    "        # checkpoints as meta data\n",
    "        cfg.checkpoint_config.meta = dict(\n",
    "            mmseg_version=f'{__version__}+{get_git_hash()[:7]}',\n",
    "            config=cfg.pretty_text,\n",
    "            CLASSES=datasets[0].CLASSES,\n",
    "            PALETTE=datasets[0].PALETTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88137f80-f5fd-4d06-9bbf-8ef1f2702578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSES: (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103)\n"
     ]
    }
   ],
   "source": [
    "model.CLASSES = datasets[0].CLASSES\n",
    "print(\"CLASSES:\", model.CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7308e8e2-0e34-4715-9c60-0d148200b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_segmentor(\n",
    "    model,\n",
    "    datasets,\n",
    "    cfg,\n",
    "    distributed=distributed,\n",
    "    validate=True,\n",
    "    timestamp=timestamp,\n",
    "    meta=meta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91b5141-505a-4e60-83fb-a798b606d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93913ca-d4cf-4938-b5fc-f2877d43960d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (Food-Detection)",
   "language": "python",
   "name": "pycharm-a66d8ed6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
